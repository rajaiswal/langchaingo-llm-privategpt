// Code generated by ogen, DO NOT EDIT.

package privategptclient

import (
	"context"
	"net/url"
	"strings"

	"github.com/go-faster/errors"

	"github.com/ogen-go/ogen/conv"
	ht "github.com/ogen-go/ogen/http"
	"github.com/ogen-go/ogen/uri"
)

// Invoker invokes operations described by OpenAPI v3 specification.
type Invoker interface {
	// ChatCompletionV1ChatCompletionsPost invokes chat_completion_v1_chat_completions_post operation.
	//
	// Given a list of messages comprising a conversation, return a response.
	// Optionally include an initial `role: system` message to influence the way
	// the LLM answers.
	// If `use_context` is set to `true`, the model will use context coming
	// from the ingested documents to create the response. The documents being used can
	// be filtered using the `context_filter` and passing the document IDs to be used.
	// Ingested documents IDs can be found using `/ingest/list` endpoint. If you want
	// all ingested documents to be used, remove `context_filter` altogether.
	// When using `'include_sources': true`, the API will return the source Chunks used
	// to create the response, which come from the context provided.
	// When using `'stream': true`, the API will return data chunks following [OpenAI's
	// streaming model](https://platform.openai.com/docs/api-reference/chat/streaming):
	// ```
	// {"id":"12345","object":"completion.chunk","created":1694268190,
	// "model":"private-gpt","choices":[{"index":0,"delta":{"content":"Hello"},
	// "finish_reason":null}]}
	// ```.
	//
	// POST /v1/chat/completions
	ChatCompletionV1ChatCompletionsPost(ctx context.Context, request *ChatBody) (ChatCompletionV1ChatCompletionsPostRes, error)
	// ChunksRetrievalV1ChunksPost invokes chunks_retrieval_v1_chunks_post operation.
	//
	// Given a `text`, returns the most relevant chunks from the ingested documents.
	// The returned information can be used to generate prompts that can be
	// passed to `/completions` or `/chat/completions` APIs. Note: it is usually a very
	// fast API, because only the Embeddings model is involved, not the LLM. The
	// returned information contains the relevant chunk `text` together with the source
	// `document` it is coming from. It also contains a score that can be used to
	// compare different results.
	// The max number of chunks to be returned is set using the `limit` param.
	// Previous and next chunks (pieces of text that appear right before or after in the
	// document) can be fetched by using the `prev_next_chunks` field.
	// The documents being used can be filtered using the `context_filter` and passing
	// the document IDs to be used. Ingested documents IDs can be found using
	// `/ingest/list` endpoint. If you want all ingested documents to be used,
	// remove `context_filter` altogether.
	//
	// POST /v1/chunks
	ChunksRetrievalV1ChunksPost(ctx context.Context, request *ChunksBody) (ChunksRetrievalV1ChunksPostRes, error)
	// DeleteIngestedV1IngestDocIDDelete invokes delete_ingested_v1_ingest__doc_id__delete operation.
	//
	// Delete the specified ingested Document.
	// The `doc_id` can be obtained from the `GET /ingest/list` endpoint.
	// The document will be effectively deleted from your storage context.
	//
	// DELETE /v1/ingest/{doc_id}
	DeleteIngestedV1IngestDocIDDelete(ctx context.Context, params DeleteIngestedV1IngestDocIDDeleteParams) (DeleteIngestedV1IngestDocIDDeleteRes, error)
	// EmbeddingsGenerationV1EmbeddingsPost invokes embeddings_generation_v1_embeddings_post operation.
	//
	// Get a vector representation of a given input.
	// That vector representation can be easily consumed
	// by machine learning models and algorithms.
	//
	// POST /v1/embeddings
	EmbeddingsGenerationV1EmbeddingsPost(ctx context.Context, request *EmbeddingsBody) (EmbeddingsGenerationV1EmbeddingsPostRes, error)
	// HealthHealthGet invokes health_health_get operation.
	//
	// Return ok if the system is up.
	//
	// GET /health
	HealthHealthGet(ctx context.Context) (*HealthResponse, error)
	// IngestFileV1IngestFilePost invokes ingest_file_v1_ingest_file_post operation.
	//
	// Ingests and processes a file, storing its chunks to be used as context.
	// The context obtained from files is later used in
	// `/chat/completions`, `/completions`, and `/chunks` APIs.
	// Most common document
	// formats are supported, but you may be prompted to install an extra dependency to
	// manage a specific file type.
	// A file can generate different Documents (for example a PDF generates one Document
	// per page). All Documents IDs are returned in the response, together with the
	// extracted Metadata (which is later used to improve context retrieval). Those IDs
	// can be used to filter the context used to create responses in
	// `/chat/completions`, `/completions`, and `/chunks` APIs.
	//
	// POST /v1/ingest/file
	IngestFileV1IngestFilePost(ctx context.Context, request *BodyIngestFileV1IngestFilePostMultipart) (IngestFileV1IngestFilePostRes, error)
	// IngestTextV1IngestTextPost invokes ingest_text_v1_ingest_text_post operation.
	//
	// Ingests and processes a text, storing its chunks to be used as context.
	// The context obtained from files is later used in
	// `/chat/completions`, `/completions`, and `/chunks` APIs.
	// A Document will be generated with the given text. The Document
	// ID is returned in the response, together with the
	// extracted Metadata (which is later used to improve context retrieval). That ID
	// can be used to filter the context used to create responses in
	// `/chat/completions`, `/completions`, and `/chunks` APIs.
	//
	// POST /v1/ingest/text
	IngestTextV1IngestTextPost(ctx context.Context, request *IngestTextBody) (IngestTextV1IngestTextPostRes, error)
	// IngestV1IngestPost invokes ingest_v1_ingest_post operation.
	//
	// Ingests and processes a file.
	// Deprecated. Use ingest/file instead.
	//
	// Deprecated: schema marks this operation as deprecated.
	//
	// POST /v1/ingest
	IngestV1IngestPost(ctx context.Context, request *BodyIngestV1IngestPostMultipart) (IngestV1IngestPostRes, error)
	// ListIngestedV1IngestListGet invokes list_ingested_v1_ingest_list_get operation.
	//
	// Lists already ingested Documents including their Document ID and metadata.
	// Those IDs can be used to filter the context used to create responses
	// in `/chat/completions`, `/completions`, and `/chunks` APIs.
	//
	// GET /v1/ingest/list
	ListIngestedV1IngestListGet(ctx context.Context) (*IngestResponse, error)
	// PromptCompletionV1CompletionsPost invokes prompt_completion_v1_completions_post operation.
	//
	// We recommend most users use our Chat completions API.
	// Given a prompt, the model will return one predicted completion.
	// Optionally include a `system_prompt` to influence the way the LLM answers.
	// If `use_context`
	// is set to `true`, the model will use context coming from the ingested documents
	// to create the response. The documents being used can be filtered using the
	// `context_filter` and passing the document IDs to be used. Ingested documents IDs
	// can be found using `/ingest/list` endpoint. If you want all ingested documents to
	// be used, remove `context_filter` altogether.
	// When using `'include_sources': true`, the API will return the source Chunks used
	// to create the response, which come from the context provided.
	// When using `'stream': true`, the API will return data chunks following [OpenAI's
	// streaming model](https://platform.openai.com/docs/api-reference/chat/streaming):
	// ```
	// {"id":"12345","object":"completion.chunk","created":1694268190,
	// "model":"private-gpt","choices":[{"index":0,"delta":{"content":"Hello"},
	// "finish_reason":null}]}
	// ```.
	//
	// POST /v1/completions
	PromptCompletionV1CompletionsPost(ctx context.Context, request *CompletionsBody) (PromptCompletionV1CompletionsPostRes, error)
}

// Client implements OAS client.
type Client struct {
	serverURL *url.URL
	baseClient
}

func trimTrailingSlashes(u *url.URL) {
	u.Path = strings.TrimRight(u.Path, "/")
	u.RawPath = strings.TrimRight(u.RawPath, "/")
}

// NewClient initializes new Client defined by OAS.
func NewClient(serverURL string, opts ...ClientOption) (*Client, error) {
	u, err := url.Parse(serverURL)
	if err != nil {
		return nil, err
	}
	trimTrailingSlashes(u)

	c, err := newClientConfig(opts...).baseClient()
	if err != nil {
		return nil, err
	}
	return &Client{
		serverURL:  u,
		baseClient: c,
	}, nil
}

type serverURLKey struct{}

// WithServerURL sets context key to override server URL.
func WithServerURL(ctx context.Context, u *url.URL) context.Context {
	return context.WithValue(ctx, serverURLKey{}, u)
}

func (c *Client) requestURL(ctx context.Context) *url.URL {
	u, ok := ctx.Value(serverURLKey{}).(*url.URL)
	if !ok {
		return c.serverURL
	}
	return u
}

// ChatCompletionV1ChatCompletionsPost invokes chat_completion_v1_chat_completions_post operation.
//
// Given a list of messages comprising a conversation, return a response.
// Optionally include an initial `role: system` message to influence the way
// the LLM answers.
// If `use_context` is set to `true`, the model will use context coming
// from the ingested documents to create the response. The documents being used can
// be filtered using the `context_filter` and passing the document IDs to be used.
// Ingested documents IDs can be found using `/ingest/list` endpoint. If you want
// all ingested documents to be used, remove `context_filter` altogether.
// When using `'include_sources': true`, the API will return the source Chunks used
// to create the response, which come from the context provided.
// When using `'stream': true`, the API will return data chunks following [OpenAI's
// streaming model](https://platform.openai.com/docs/api-reference/chat/streaming):
// ```
// {"id":"12345","object":"completion.chunk","created":1694268190,
// "model":"private-gpt","choices":[{"index":0,"delta":{"content":"Hello"},
// "finish_reason":null}]}
// ```.
//
// POST /v1/chat/completions
func (c *Client) ChatCompletionV1ChatCompletionsPost(ctx context.Context, request *ChatBody) (ChatCompletionV1ChatCompletionsPostRes, error) {
	res, err := c.sendChatCompletionV1ChatCompletionsPost(ctx, request)
	return res, err
}

func (c *Client) sendChatCompletionV1ChatCompletionsPost(ctx context.Context, request *ChatBody) (res ChatCompletionV1ChatCompletionsPostRes, err error) {

	u := uri.Clone(c.requestURL(ctx))
	var pathParts [1]string
	pathParts[0] = "/v1/chat/completions"
	uri.AddPathParts(u, pathParts[:]...)

	r, err := ht.NewRequest(ctx, "POST", u)
	if err != nil {
		return res, errors.Wrap(err, "create request")
	}
	if err := encodeChatCompletionV1ChatCompletionsPostRequest(request, r); err != nil {
		return res, errors.Wrap(err, "encode request")
	}

	resp, err := c.cfg.Client.Do(r)
	if err != nil {
		return res, errors.Wrap(err, "do request")
	}
	defer resp.Body.Close()

	result, err := decodeChatCompletionV1ChatCompletionsPostResponse(resp)
	if err != nil {
		return res, errors.Wrap(err, "decode response")
	}

	return result, nil
}

// ChunksRetrievalV1ChunksPost invokes chunks_retrieval_v1_chunks_post operation.
//
// Given a `text`, returns the most relevant chunks from the ingested documents.
// The returned information can be used to generate prompts that can be
// passed to `/completions` or `/chat/completions` APIs. Note: it is usually a very
// fast API, because only the Embeddings model is involved, not the LLM. The
// returned information contains the relevant chunk `text` together with the source
// `document` it is coming from. It also contains a score that can be used to
// compare different results.
// The max number of chunks to be returned is set using the `limit` param.
// Previous and next chunks (pieces of text that appear right before or after in the
// document) can be fetched by using the `prev_next_chunks` field.
// The documents being used can be filtered using the `context_filter` and passing
// the document IDs to be used. Ingested documents IDs can be found using
// `/ingest/list` endpoint. If you want all ingested documents to be used,
// remove `context_filter` altogether.
//
// POST /v1/chunks
func (c *Client) ChunksRetrievalV1ChunksPost(ctx context.Context, request *ChunksBody) (ChunksRetrievalV1ChunksPostRes, error) {
	res, err := c.sendChunksRetrievalV1ChunksPost(ctx, request)
	return res, err
}

func (c *Client) sendChunksRetrievalV1ChunksPost(ctx context.Context, request *ChunksBody) (res ChunksRetrievalV1ChunksPostRes, err error) {

	u := uri.Clone(c.requestURL(ctx))
	var pathParts [1]string
	pathParts[0] = "/v1/chunks"
	uri.AddPathParts(u, pathParts[:]...)

	r, err := ht.NewRequest(ctx, "POST", u)
	if err != nil {
		return res, errors.Wrap(err, "create request")
	}
	if err := encodeChunksRetrievalV1ChunksPostRequest(request, r); err != nil {
		return res, errors.Wrap(err, "encode request")
	}

	resp, err := c.cfg.Client.Do(r)
	if err != nil {
		return res, errors.Wrap(err, "do request")
	}
	defer resp.Body.Close()

	result, err := decodeChunksRetrievalV1ChunksPostResponse(resp)
	if err != nil {
		return res, errors.Wrap(err, "decode response")
	}

	return result, nil
}

// DeleteIngestedV1IngestDocIDDelete invokes delete_ingested_v1_ingest__doc_id__delete operation.
//
// Delete the specified ingested Document.
// The `doc_id` can be obtained from the `GET /ingest/list` endpoint.
// The document will be effectively deleted from your storage context.
//
// DELETE /v1/ingest/{doc_id}
func (c *Client) DeleteIngestedV1IngestDocIDDelete(ctx context.Context, params DeleteIngestedV1IngestDocIDDeleteParams) (DeleteIngestedV1IngestDocIDDeleteRes, error) {
	res, err := c.sendDeleteIngestedV1IngestDocIDDelete(ctx, params)
	return res, err
}

func (c *Client) sendDeleteIngestedV1IngestDocIDDelete(ctx context.Context, params DeleteIngestedV1IngestDocIDDeleteParams) (res DeleteIngestedV1IngestDocIDDeleteRes, err error) {

	u := uri.Clone(c.requestURL(ctx))
	var pathParts [2]string
	pathParts[0] = "/v1/ingest/"
	{
		// Encode "doc_id" parameter.
		e := uri.NewPathEncoder(uri.PathEncoderConfig{
			Param:   "doc_id",
			Style:   uri.PathStyleSimple,
			Explode: false,
		})
		if err := func() error {
			return e.EncodeValue(conv.StringToString(params.DocID))
		}(); err != nil {
			return res, errors.Wrap(err, "encode path")
		}
		encoded, err := e.Result()
		if err != nil {
			return res, errors.Wrap(err, "encode path")
		}
		pathParts[1] = encoded
	}
	uri.AddPathParts(u, pathParts[:]...)

	r, err := ht.NewRequest(ctx, "DELETE", u)
	if err != nil {
		return res, errors.Wrap(err, "create request")
	}

	resp, err := c.cfg.Client.Do(r)
	if err != nil {
		return res, errors.Wrap(err, "do request")
	}
	defer resp.Body.Close()

	result, err := decodeDeleteIngestedV1IngestDocIDDeleteResponse(resp)
	if err != nil {
		return res, errors.Wrap(err, "decode response")
	}

	return result, nil
}

// EmbeddingsGenerationV1EmbeddingsPost invokes embeddings_generation_v1_embeddings_post operation.
//
// Get a vector representation of a given input.
// That vector representation can be easily consumed
// by machine learning models and algorithms.
//
// POST /v1/embeddings
func (c *Client) EmbeddingsGenerationV1EmbeddingsPost(ctx context.Context, request *EmbeddingsBody) (EmbeddingsGenerationV1EmbeddingsPostRes, error) {
	res, err := c.sendEmbeddingsGenerationV1EmbeddingsPost(ctx, request)
	return res, err
}

func (c *Client) sendEmbeddingsGenerationV1EmbeddingsPost(ctx context.Context, request *EmbeddingsBody) (res EmbeddingsGenerationV1EmbeddingsPostRes, err error) {

	u := uri.Clone(c.requestURL(ctx))
	var pathParts [1]string
	pathParts[0] = "/v1/embeddings"
	uri.AddPathParts(u, pathParts[:]...)

	r, err := ht.NewRequest(ctx, "POST", u)
	if err != nil {
		return res, errors.Wrap(err, "create request")
	}
	if err := encodeEmbeddingsGenerationV1EmbeddingsPostRequest(request, r); err != nil {
		return res, errors.Wrap(err, "encode request")
	}

	resp, err := c.cfg.Client.Do(r)
	if err != nil {
		return res, errors.Wrap(err, "do request")
	}
	defer resp.Body.Close()

	result, err := decodeEmbeddingsGenerationV1EmbeddingsPostResponse(resp)
	if err != nil {
		return res, errors.Wrap(err, "decode response")
	}

	return result, nil
}

// HealthHealthGet invokes health_health_get operation.
//
// Return ok if the system is up.
//
// GET /health
func (c *Client) HealthHealthGet(ctx context.Context) (*HealthResponse, error) {
	res, err := c.sendHealthHealthGet(ctx)
	return res, err
}

func (c *Client) sendHealthHealthGet(ctx context.Context) (res *HealthResponse, err error) {

	u := uri.Clone(c.requestURL(ctx))
	var pathParts [1]string
	pathParts[0] = "/health"
	uri.AddPathParts(u, pathParts[:]...)

	r, err := ht.NewRequest(ctx, "GET", u)
	if err != nil {
		return res, errors.Wrap(err, "create request")
	}

	resp, err := c.cfg.Client.Do(r)
	if err != nil {
		return res, errors.Wrap(err, "do request")
	}
	defer resp.Body.Close()

	result, err := decodeHealthHealthGetResponse(resp)
	if err != nil {
		return res, errors.Wrap(err, "decode response")
	}

	return result, nil
}

// IngestFileV1IngestFilePost invokes ingest_file_v1_ingest_file_post operation.
//
// Ingests and processes a file, storing its chunks to be used as context.
// The context obtained from files is later used in
// `/chat/completions`, `/completions`, and `/chunks` APIs.
// Most common document
// formats are supported, but you may be prompted to install an extra dependency to
// manage a specific file type.
// A file can generate different Documents (for example a PDF generates one Document
// per page). All Documents IDs are returned in the response, together with the
// extracted Metadata (which is later used to improve context retrieval). Those IDs
// can be used to filter the context used to create responses in
// `/chat/completions`, `/completions`, and `/chunks` APIs.
//
// POST /v1/ingest/file
func (c *Client) IngestFileV1IngestFilePost(ctx context.Context, request *BodyIngestFileV1IngestFilePostMultipart) (IngestFileV1IngestFilePostRes, error) {
	res, err := c.sendIngestFileV1IngestFilePost(ctx, request)
	return res, err
}

func (c *Client) sendIngestFileV1IngestFilePost(ctx context.Context, request *BodyIngestFileV1IngestFilePostMultipart) (res IngestFileV1IngestFilePostRes, err error) {

	u := uri.Clone(c.requestURL(ctx))
	var pathParts [1]string
	pathParts[0] = "/v1/ingest/file"
	uri.AddPathParts(u, pathParts[:]...)

	r, err := ht.NewRequest(ctx, "POST", u)
	if err != nil {
		return res, errors.Wrap(err, "create request")
	}
	if err := encodeIngestFileV1IngestFilePostRequest(request, r); err != nil {
		return res, errors.Wrap(err, "encode request")
	}

	resp, err := c.cfg.Client.Do(r)
	if err != nil {
		return res, errors.Wrap(err, "do request")
	}
	defer resp.Body.Close()

	result, err := decodeIngestFileV1IngestFilePostResponse(resp)
	if err != nil {
		return res, errors.Wrap(err, "decode response")
	}

	return result, nil
}

// IngestTextV1IngestTextPost invokes ingest_text_v1_ingest_text_post operation.
//
// Ingests and processes a text, storing its chunks to be used as context.
// The context obtained from files is later used in
// `/chat/completions`, `/completions`, and `/chunks` APIs.
// A Document will be generated with the given text. The Document
// ID is returned in the response, together with the
// extracted Metadata (which is later used to improve context retrieval). That ID
// can be used to filter the context used to create responses in
// `/chat/completions`, `/completions`, and `/chunks` APIs.
//
// POST /v1/ingest/text
func (c *Client) IngestTextV1IngestTextPost(ctx context.Context, request *IngestTextBody) (IngestTextV1IngestTextPostRes, error) {
	res, err := c.sendIngestTextV1IngestTextPost(ctx, request)
	return res, err
}

func (c *Client) sendIngestTextV1IngestTextPost(ctx context.Context, request *IngestTextBody) (res IngestTextV1IngestTextPostRes, err error) {

	u := uri.Clone(c.requestURL(ctx))
	var pathParts [1]string
	pathParts[0] = "/v1/ingest/text"
	uri.AddPathParts(u, pathParts[:]...)

	r, err := ht.NewRequest(ctx, "POST", u)
	if err != nil {
		return res, errors.Wrap(err, "create request")
	}
	if err := encodeIngestTextV1IngestTextPostRequest(request, r); err != nil {
		return res, errors.Wrap(err, "encode request")
	}

	resp, err := c.cfg.Client.Do(r)
	if err != nil {
		return res, errors.Wrap(err, "do request")
	}
	defer resp.Body.Close()

	result, err := decodeIngestTextV1IngestTextPostResponse(resp)
	if err != nil {
		return res, errors.Wrap(err, "decode response")
	}

	return result, nil
}

// IngestV1IngestPost invokes ingest_v1_ingest_post operation.
//
// Ingests and processes a file.
// Deprecated. Use ingest/file instead.
//
// Deprecated: schema marks this operation as deprecated.
//
// POST /v1/ingest
func (c *Client) IngestV1IngestPost(ctx context.Context, request *BodyIngestV1IngestPostMultipart) (IngestV1IngestPostRes, error) {
	res, err := c.sendIngestV1IngestPost(ctx, request)
	return res, err
}

func (c *Client) sendIngestV1IngestPost(ctx context.Context, request *BodyIngestV1IngestPostMultipart) (res IngestV1IngestPostRes, err error) {

	u := uri.Clone(c.requestURL(ctx))
	var pathParts [1]string
	pathParts[0] = "/v1/ingest"
	uri.AddPathParts(u, pathParts[:]...)

	r, err := ht.NewRequest(ctx, "POST", u)
	if err != nil {
		return res, errors.Wrap(err, "create request")
	}
	if err := encodeIngestV1IngestPostRequest(request, r); err != nil {
		return res, errors.Wrap(err, "encode request")
	}

	resp, err := c.cfg.Client.Do(r)
	if err != nil {
		return res, errors.Wrap(err, "do request")
	}
	defer resp.Body.Close()

	result, err := decodeIngestV1IngestPostResponse(resp)
	if err != nil {
		return res, errors.Wrap(err, "decode response")
	}

	return result, nil
}

// ListIngestedV1IngestListGet invokes list_ingested_v1_ingest_list_get operation.
//
// Lists already ingested Documents including their Document ID and metadata.
// Those IDs can be used to filter the context used to create responses
// in `/chat/completions`, `/completions`, and `/chunks` APIs.
//
// GET /v1/ingest/list
func (c *Client) ListIngestedV1IngestListGet(ctx context.Context) (*IngestResponse, error) {
	res, err := c.sendListIngestedV1IngestListGet(ctx)
	return res, err
}

func (c *Client) sendListIngestedV1IngestListGet(ctx context.Context) (res *IngestResponse, err error) {

	u := uri.Clone(c.requestURL(ctx))
	var pathParts [1]string
	pathParts[0] = "/v1/ingest/list"
	uri.AddPathParts(u, pathParts[:]...)

	r, err := ht.NewRequest(ctx, "GET", u)
	if err != nil {
		return res, errors.Wrap(err, "create request")
	}

	resp, err := c.cfg.Client.Do(r)
	if err != nil {
		return res, errors.Wrap(err, "do request")
	}
	defer resp.Body.Close()

	result, err := decodeListIngestedV1IngestListGetResponse(resp)
	if err != nil {
		return res, errors.Wrap(err, "decode response")
	}

	return result, nil
}

// PromptCompletionV1CompletionsPost invokes prompt_completion_v1_completions_post operation.
//
// We recommend most users use our Chat completions API.
// Given a prompt, the model will return one predicted completion.
// Optionally include a `system_prompt` to influence the way the LLM answers.
// If `use_context`
// is set to `true`, the model will use context coming from the ingested documents
// to create the response. The documents being used can be filtered using the
// `context_filter` and passing the document IDs to be used. Ingested documents IDs
// can be found using `/ingest/list` endpoint. If you want all ingested documents to
// be used, remove `context_filter` altogether.
// When using `'include_sources': true`, the API will return the source Chunks used
// to create the response, which come from the context provided.
// When using `'stream': true`, the API will return data chunks following [OpenAI's
// streaming model](https://platform.openai.com/docs/api-reference/chat/streaming):
// ```
// {"id":"12345","object":"completion.chunk","created":1694268190,
// "model":"private-gpt","choices":[{"index":0,"delta":{"content":"Hello"},
// "finish_reason":null}]}
// ```.
//
// POST /v1/completions
func (c *Client) PromptCompletionV1CompletionsPost(ctx context.Context, request *CompletionsBody) (PromptCompletionV1CompletionsPostRes, error) {
	res, err := c.sendPromptCompletionV1CompletionsPost(ctx, request)
	return res, err
}

func (c *Client) sendPromptCompletionV1CompletionsPost(ctx context.Context, request *CompletionsBody) (res PromptCompletionV1CompletionsPostRes, err error) {

	u := uri.Clone(c.requestURL(ctx))
	var pathParts [1]string
	pathParts[0] = "/v1/completions"
	uri.AddPathParts(u, pathParts[:]...)

	r, err := ht.NewRequest(ctx, "POST", u)
	if err != nil {
		return res, errors.Wrap(err, "create request")
	}
	if err := encodePromptCompletionV1CompletionsPostRequest(request, r); err != nil {
		return res, errors.Wrap(err, "encode request")
	}

	resp, err := c.cfg.Client.Do(r)
	if err != nil {
		return res, errors.Wrap(err, "do request")
	}
	defer resp.Body.Close()

	result, err := decodePromptCompletionV1CompletionsPostResponse(resp)
	if err != nil {
		return res, errors.Wrap(err, "decode response")
	}

	return result, nil
}
